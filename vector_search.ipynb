{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Этап 1: Векторный поиск\n",
    "\n",
    "## Описание\n",
    "Имеется структурированная база знаний, содержащая инструкции и правила по взаимодействию оператора Пункта выдачи заказов (ПВЗ) с платформой, клиентом, товарами и описания других бизнес-процессов.\n",
    "\n",
    "Необходимо реализовать систему, которая по вопросу оператора ПВЗ будет находить релевантные фрагменты базы знаний.\n",
    "\n",
    "## Исходные данные\n",
    "1. **chunks (1).csv** - база знаний, разбитая на чанки\n",
    "   - Содержит заголовок и содержание каждого фрагмента\n",
    "2. **train_data.csv** - размеченные тренировочные данные\n",
    "   - Содержит вопросы операторов и соответствующие им чанки\n",
    "\n",
    "### Метрики качества:\n",
    "- recall@1\n",
    "- recall@3\n",
    "- recall@5\n",
    "\n",
    "## План реализации проекта\n",
    "\n",
    "### 1. Подготовка данных\n",
    "   - Загрузка и первичный анализ данных\n",
    "   - Предобработка текста\n",
    "\n",
    "### 2. Разработка базовой модели\n",
    "   - Реализация TF-IDF векторизации\n",
    "   - Создание функции косинусного сходства\n",
    "   - Базовая оценка качества\n",
    "\n",
    "### 3. Улучшение модели\n",
    "   - Эксперименты с различными методами векторизации\n",
    "   - Оценка качества и сравнение\n",
    "   - Применение функции сравнения нового вопроса с уже имеющимися\n",
    "\n",
    "### 4. Создание сервиса\n",
    "   - Разработка класса ChunkFinder\n",
    "   - Реализация FastAPI для класса ChunkFinder по поиску чанков на вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Загрузим стоп слова для обработки русской речи\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Инициализация лемматизатора\n",
    "mystem = Mystem()\n",
    "# Инициализация векторизатора\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "chunks_df = pd.read_csv('chunks (1).csv')\n",
    "train_df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрю как выглядит содержимое\n",
    "print(chunks_df.shape)\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка содержимого\n",
    "\n",
    "Видно, что у нас текст имеет разные регистры, разные формы речи, местоимения. \n",
    "\n",
    "Все эти особенности речи влияют в отрациательную стоону для поиска по вектрной схожести, таким образом нужно все слова привести к нижнему регистру, нужно провести лемматизацию для получения стандартной морфологии слова, а так же удаления местоимений и лишних знаков. \n",
    "\n",
    "Я протестирую 2 версии по качеству векторизации с объединением заголовков и без. С одной строны заголовки несут в себе контекст и ключевые слова, с другой стороны они могут повторяться. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ф-ция для стандартной очистки слов в тексте\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Приведем к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удалим спец. символы\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаляем стоп-слова\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords]\n",
    "    # Лемматизация\n",
    "    lemmatized_tokens = mystem.lemmatize(' '.join(tokens))\n",
    "    # Удаляем пробелы и пустые строки после лемматизации\n",
    "    lemmatized_tokens = [token.strip() for token in lemmatized_tokens if token.strip()]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создам столбец в chunks_df, который будет содержать заголовок и чанк для полноты поиска\n",
    "chunks_df['full_text'] = chunks_df['Headers'] + ' ' + chunks_df['Chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим преобразованный столбец\n",
    "chunks_df['processed_text'] = chunks_df['full_text'].apply(preprocess_text)\n",
    "# chunks_df['processed_text'] = chunks_df['Chunk'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценим результат\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Разработка базовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем TF-IDF матрицу для чанков\n",
    "chunks_vectors = vectorizer.fit_transform(chunks_df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ф-ция для оценки на основе косинусного сходства\n",
    "def find_relevant_chunks(question: str, top_k: int=5) -> list:\n",
    "    # Предобработка вопроса\n",
    "    processed_question = preprocess_text(question)\n",
    "    # Векторизуем вопрос\n",
    "    question_vector = vectorizer.transform([processed_question])\n",
    "    # Вычисляем косинусное сходство\n",
    "    similarities = cosine_similarity(question_vector, chunks_vectors).flatten()\n",
    "    # Получаем индексы top_k наиболее релевантных чанков\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    # Формируем результат\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'header': chunks_df.iloc[idx]['Headers'],\n",
    "            'chunk': chunks_df.iloc[idx]['Chunk'],\n",
    "            'similarity_score': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для оценки модели\n",
    "def evaluate_model(test_data: pd.DataFrame, k_values: list=[1, 3, 5]) -> Dict[str, float]:\n",
    "    recalls = {k: [] for k in k_values}\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        question = row['Question']\n",
    "        correct_chunk = row['Chunk']\n",
    "        \n",
    "        results = find_relevant_chunks(question, max(k_values))\n",
    "        \n",
    "        for k in k_values:\n",
    "            top_k_chunks = [r['chunk'] for r in results[:k]]\n",
    "            recall = 1 if correct_chunk in top_k_chunks else 0\n",
    "            recalls[k].append(recall)\n",
    "    \n",
    "    return {f'recall@{k}': np.mean(recalls[k]) for k in k_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.2714285714285714),\n",
       " 'recall@3': np.float64(0.45285714285714285),\n",
       " 'recall@5': np.float64(0.5342857142857143)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.2714285714285714),\n",
       " 'recall@3': np.float64(0.45285714285714285),\n",
       " 'recall@5': np.float64(0.5342857142857143)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# headers + chunks + edit lemma + processed_question\n",
    "{'recall@1': np.float64(0.2714285714285714),\n",
    " 'recall@3': np.float64(0.45285714285714285),\n",
    " 'recall@5': np.float64(0.5342857142857143)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.22),\n",
       " 'recall@3': np.float64(0.37714285714285717),\n",
       " 'recall@5': np.float64(0.47285714285714286)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunks\n",
    "{'recall@1': np.float64(0.22),\n",
    " 'recall@3': np.float64(0.37714285714285717),\n",
    " 'recall@5': np.float64(0.47285714285714286)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Улучшение модели. Попробую BERT в надежде улучшить качество модели\n",
    "\n",
    "BERT и TF-IDF работают по-разному:\n",
    "\n",
    "- BERT понимает семантический смысл и контекст\n",
    "- TF-IDF работает на уровне частоты слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка данных\n",
    "chunks_df = pd.read_csv('chunks (1).csv')\n",
    "train_df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем многоязычную модель BERT\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Текст с заголовком\n",
    "chunks_df['full_text'] = chunks_df['Headers'] + ' ' + chunks_df['Chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:09<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Предварительно вычисляем эмбеддинги\n",
    "embeddings = model.encode(chunks_df['full_text'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_chunks(question, top_k=5):\n",
    "    # Получаем эмбеддинг вопроса\n",
    "    question_embedding = model.encode([question])\n",
    "    \n",
    "    # Вычисляем косинусное сходство\n",
    "    similarities = cosine_similarity(question_embedding, embeddings).flatten()\n",
    "    \n",
    "    # Получаем топ-k результатов\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'header': chunks_df.iloc[idx]['Headers'],\n",
    "            'chunk': chunks_df.iloc[idx]['Chunk'],\n",
    "            'similarity_score': float(similarities[idx])\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.21714285714285714),\n",
       " 'recall@3': np.float64(0.3757142857142857),\n",
       " 'recall@5': np.float64(0.4471428571428571)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто чанки\n",
    "{'recall@1': np.float64(0.2042857142857143),\n",
    " 'recall@3': np.float64(0.3357142857142857),\n",
    " 'recall@5': np.float64(0.3942857142857143)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто заголовки и чанки\n",
    "{'recall@1': np.float64(0.21714285714285714),\n",
    " 'recall@3': np.float64(0.3757142857142857),\n",
    " 'recall@5': np.float64(0.4471428571428571)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.18714285714285714),\n",
       " 'recall@3': np.float64(0.29285714285714287),\n",
       " 'recall@5': np.float64(0.37)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Улучшенное объединение текста с весами, удвоенная важность заголовков\n",
    "{'recall@1': np.float64(0.18714285714285714),\n",
    " 'recall@3': np.float64(0.29285714285714287),\n",
    " 'recall@5': np.float64(0.37)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Еще попробую модель от sbert_large_nlu_ru нашел на habr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Загрузка данных\n",
    "chunks_df = pd.read_csv('chunks (1).csv')\n",
    "train_df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Текст с заголовком\n",
    "chunks_df['full_text'] = chunks_df['Headers'] + ' ' + chunks_df['Chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model.cuda()  # GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ф-ция для получения эмбеддингов через BERT\n",
    "def embed_bert_cls(texts, model, tokenizer, max_length=512):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        t = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "        embedding = model_output.last_hidden_state[:, 0, :]\n",
    "        embedding = torch.nn.functional.normalize(embedding)\n",
    "        embeddings.append(embedding[0].cpu().numpy())\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_bert_cls(chunks_df['full_text'].to_list(), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ф-ция для оценки на основе косинусного сходства между вопросом и чанками\n",
    "def find_relevant_chunks(question, top_k=5):\n",
    "    # Получаем эмбеддинг вопроса\n",
    "    question_embedding = embed_bert_cls([question], model, tokenizer)[0]\n",
    "    \n",
    "    # Вычисляем косинусное сходство\n",
    "    similarities = cosine_similarity([question_embedding], embeddings).flatten()\n",
    "    \n",
    "    # Получаем топ-k результатов\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'header': chunks_df.iloc[idx]['Headers'],\n",
    "            'chunk': chunks_df.iloc[idx]['Chunk'],\n",
    "            'similarity_score': float(similarities[idx])\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.02142857142857143),\n",
       " 'recall@3': np.float64(0.054285714285714284),\n",
       " 'recall@5': np.float64(0.08)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат гораздо хуже чем у первой модели, буду работать с вариантом 3.1. Протестирую вариант с очисткой текста и лемматизацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Улучшение модели из 3.1. Попробую скомбинировать\n",
    "\n",
    "Веса 0.7 и 0.3 дают преимущество BERT, потому что:\n",
    " - BERT лучше улавливает смысловые связи\n",
    " - TF-IDF помогает \"заякорить\" результат на ключевых словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem\\Desktop\\DS\\Norvik_workplace\\vector_search\\.venvwin\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Загрузка данных\n",
    "chunks_df = pd.read_csv('chunks (1).csv')\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Используем многоязычную модель BERT\n",
    "bert_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "# TF-IDF векторайзер\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.95, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df[['Headers', 'Chunk']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ф-ция для стандартной очистки слов в тексте\n",
    "def preprocess_text_tfidf(text: str) -> str:\n",
    "    # Приведем к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удалим спец. символы\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаляем стоп-слова\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords]\n",
    "    # Лемматизация\n",
    "    lemmatized_tokens = mystem.lemmatize(' '.join(tokens))\n",
    "    # Удаляем пробелы и пустые строки после лемматизации\n",
    "    lemmatized_tokens = [token.strip() for token in lemmatized_tokens if token.strip()]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:09<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks_df['full_text'] = chunks_df['Headers'] + ' ' + chunks_df['Chunk']\n",
    "# Получим преобразованный столбец\n",
    "chunks_df['processed_text'] = chunks_df['full_text'].apply(preprocess_text_tfidf)\n",
    "\n",
    "# Создаем TF-IDF матрицу для чанков\n",
    "embeddings_tfidf = tfidf.fit_transform(chunks_df['processed_text'])\n",
    "# Вычисляем эмбеддинги для BERT\n",
    "embeddings_bert = bert_model.encode(chunks_df['full_text'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_chunks(question: str, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Улучшенная версия поиска с сохранением эффективного подхода\n",
    "    \n",
    "    Аргументы:\n",
    "        question: вопрос для поиска\n",
    "        top_k: количество возвращаемых результатов\n",
    "        bert_weight: вес для BERT скора\n",
    "        tfidf_weight: вес для TF-IDF скора\n",
    "    \"\"\"\n",
    "    # BERT similarities с нормализацией\n",
    "    question_bert_emb = bert_model.encode([question])\n",
    "    bert_similarities = cosine_similarity(question_bert_emb, embeddings_bert).flatten()\n",
    "    \n",
    "    # TF-IDF similarities с предобработкой\n",
    "    processed_question = preprocess_text_tfidf(question)\n",
    "    question_tfidf = tfidf.transform([processed_question])\n",
    "    tfidf_similarities = cosine_similarity(question_tfidf, embeddings_tfidf).flatten()\n",
    "    \n",
    "    # Комбинируем scores с весами\n",
    "    combined_similarities = (bert_similarities + tfidf_similarities)\n",
    "    \n",
    "    # Получаем топ результаты\n",
    "    top_indices = combined_similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Формируем результаты с дополнительной информацией\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        bert_contribution = bert_similarities[idx]\n",
    "        tfidf_contribution = tfidf_similarities[idx]\n",
    "        \n",
    "        results.append({\n",
    "            'header': chunks_df.iloc[idx]['Headers'],\n",
    "            'chunk': chunks_df.iloc[idx]['Chunk'],\n",
    "            'similarity_score': float(combined_similarities[idx]),\n",
    "            'bert_score': float(bert_similarities[idx]),\n",
    "            'tfidf_score': float(tfidf_similarities[idx]),\n",
    "            'bert_contribution': float(bert_contribution),\n",
    "            'tfidf_contribution': float(tfidf_contribution)\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': np.float64(0.29),\n",
       " 'recall@3': np.float64(0.5071428571428571),\n",
       " 'recall@5': np.float64(0.5771428571428572)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 0.7 * bert_similarities + 0.3 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.2842857142857143),\n",
    " 'recall@3': np.float64(0.4614285714285714),\n",
    " 'recall@5': np.float64(0.5471428571428572)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не комбинированный подход, просто выбор лучших результатов\n",
    "{'recall@1': np.float64(0.21857142857142858),\n",
    " 'recall@3': np.float64(0.37714285714285717),\n",
    " 'recall@5': np.float64(0.45)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 1 * bert_similarities + 1 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.29),\n",
    " 'recall@3': np.float64(0.5071428571428571),\n",
    " 'recall@5': np.float64(0.5771428571428572)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 0.3 * bert_similarities + 0.7 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.2814285714285714),\n",
    " 'recall@3': np.float64(0.5085714285714286),\n",
    " 'recall@5': np.float64(0.5857142857142857)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 0.8 * bert_similarities + 1 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.2842857142857143),\n",
    " 'recall@3': np.float64(0.5085714285714286),\n",
    " 'recall@5': np.float64(0.5842857142857143)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 0.6 * bert_similarities + 1 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.2785714285714286),\n",
    " 'recall@3': np.float64(0.5114285714285715),\n",
    " 'recall@5': np.float64(0.5985714285714285)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 0.4 * bert_similarities + 1 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.2785714285714286),\n",
    " 'recall@3': np.float64(0.5042857142857143),\n",
    " 'recall@5': np.float64(0.5928571428571429)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# комбинированный подход combined_similarities = 0.9 * bert_similarities + 1 * tfidf_similarities\n",
    "{'recall@1': np.float64(0.28714285714285714),\n",
    " 'recall@3': np.float64(0.5042857142857143),\n",
    " 'recall@5': np.float64(0.5814285714285714)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса не дают сильного эффекта но усложняют логику. Лучше использовать простой подход.\n",
    "\n",
    "Так же я попробую использовать имеющиеся вопросы.\n",
    "Я думаю, что если собрать хорошую выборку с вопросами, то она уже будет содержать в себе ответы на 90% вопросов. \n",
    "Так что на вопрос по тестовой выборке я буду искать вопросы в тренировочном датасете, а уже потом подключать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание эмбеддингов для тренировочных вопросов\n",
    "train_questions_bert = bert_model.encode(train_df['Question'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_training_question(self, question: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Поиск похожего вопроса в тренировочной выборке\n",
    "\n",
    "    Аргументы:\n",
    "        question: текст вопроса\n",
    "        \n",
    "    Возвращает:\n",
    "        tuple: (индекс найденного вопроса, значение схожести) или (None, 0)\n",
    "    \"\"\"\n",
    "    # Получаем эмбеддинг для нового вопроса\n",
    "    question_embedding = self.bert_model.encode([question])\n",
    "    \n",
    "    # Вычисляем схожесть с тренировочными вопросами\n",
    "    similarities = cosine_similarity(question_embedding, self.train_questions_bert)[0]\n",
    "    \n",
    "    # Находим максимальную схожесть и соответствующий индекс\n",
    "    max_similarity = np.max(similarities)\n",
    "    max_similarity_idx = np.argmax(similarities)\n",
    "    \n",
    "    # Если схожесть выше порога, возвращаем результат\n",
    "    if max_similarity >= self.bert_similarity_threshold:\n",
    "        return max_similarity_idx, max_similarity\n",
    "        \n",
    "    return None, max_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это и будет мой финальный вариант. Таким образом это комбинированный варант. \n",
    "\n",
    "1) Поиск по максимально похожему вопросу из train, \n",
    "\n",
    "2) поиска по семантике BERT а так же буквальному совпадению по словам это tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
